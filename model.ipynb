{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94449d0-1cf8-4691-ac47-b110e2eb2764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2393a0b-08c1-429e-8eac-4f395d284f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9f17be8-d9dd-4664-9755-171f83d2da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aa125753-f66c-4006-9cef-c124ce9396d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "552fab26-1a3d-4197-be95-e1df22eb4925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1b1f3720-fce3-4ed8-bf40-3d62b6a4e07e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     vidname  frame_1  frame_2  frame_3  frame_4  frame_5  frame_6  frame_7  \\\n",
      "0     000787        0        0        0        0        0        0        0   \n",
      "1     001655        0        0        0        0        0        0        0   \n",
      "2     001467        0        0        0        0        0        0        0   \n",
      "3     002686        0        0        0        0        0        0        0   \n",
      "4     001003        0        0        0        0        0        0        0   \n",
      "...      ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "1494    1495        0        0        0        0        0        0        0   \n",
      "1495    1496        0        0        0        0        0        0        0   \n",
      "1496    1497        0        0        0        0        0        0        0   \n",
      "1497    1498        0        0        0        0        0        0        0   \n",
      "1499    1500        0        0        0        0        0        0        0   \n",
      "\n",
      "      frame_8  frame_9  ...  frame_41  frame_42  frame_43  frame_44  frame_45  \\\n",
      "0           0        0  ...         0         0         0         0         0   \n",
      "1           0        0  ...         0         0         0         0         0   \n",
      "2           0        0  ...         0         0         0         0         0   \n",
      "3           0        0  ...         0         0         0         0         0   \n",
      "4           0        0  ...         0         0         0         0         0   \n",
      "...       ...      ...  ...       ...       ...       ...       ...       ...   \n",
      "1494        0        0  ...         1         1         1         1         1   \n",
      "1495        0        0  ...         0         0         0         0         0   \n",
      "1496        0        0  ...         0         0         0         0         0   \n",
      "1497        0        0  ...         0         0         0         0         0   \n",
      "1499        0        0  ...         0         0         0         0         0   \n",
      "\n",
      "      frame_46  frame_47  frame_48  frame_49  frame_50  \n",
      "0            0         0         0         0         0  \n",
      "1            0         0         0         0         0  \n",
      "2            0         0         0         0         0  \n",
      "3            0         0         0         0         0  \n",
      "4            0         0         0         0         0  \n",
      "...        ...       ...       ...       ...       ...  \n",
      "1494         1         1         1         1         1  \n",
      "1495         1         1         1         1         1  \n",
      "1496         0         0         0         1         1  \n",
      "1497         0         0         1         1         1  \n",
      "1499         1         1         1         1         1  \n",
      "\n",
      "[3802 rows x 51 columns]\n",
      "     vidname  frame_50\n",
      "0     000787         0\n",
      "1     001655         0\n",
      "2     001467         0\n",
      "3     002686         0\n",
      "4     001003         0\n",
      "...      ...       ...\n",
      "1494    1495         1\n",
      "1495    1496         1\n",
      "1496    1497         1\n",
      "1497    1498         1\n",
      "1499    1500         1\n",
      "\n",
      "[3802 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#preparing data for splitting\n",
    "\n",
    "data_new = pd.read_csv('Crash_Table.csv')\n",
    "row, col = data_new.shape\n",
    "\n",
    "for index, row in data_new.iterrows():\n",
    "    if row[\"egoinvolve\"] == \"No\":\n",
    "        data_new = data_new.drop(index)\n",
    "data_new = data_new.drop(\"startframe\", axis='columns')\n",
    "data_new = data_new.drop(\"youtubeID\", axis='columns')\n",
    "data_new = data_new.drop(\"timing\", axis='columns')\n",
    "data_new = data_new.drop(\"weather\", axis='columns')\n",
    "data_new = data_new.drop(\"egoinvolve\", axis='columns')\n",
    "\n",
    "names = os.listdir(\"./normalframes\")\n",
    "header = list(data_new.columns)\n",
    "data_normal = pd.DataFrame(columns=header)\n",
    "\n",
    "for i in names:\n",
    "    data_normal.loc[len(data_normal.index)] = [i, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "data_new = pd.concat([data_normal, data_new], axis=0)\n",
    "print(data_new)\n",
    "data_new.drop(data_new.iloc[:, 1:50], inplace=True, axis=1)\n",
    "print(data_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "33d9c716-a19c-4dab-92f5-fa53b960784d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     vidname  frame_50\n",
      "105      106         1\n",
      "1138    1139         1\n",
      "1841  001465         0\n",
      "1838  000543         0\n",
      "2649  000798         0\n",
      "...      ...       ...\n",
      "2086  002960         0\n",
      "639      640         1\n",
      "2482  001930         0\n",
      "80    000745         0\n",
      "1793  002473         0\n",
      "\n",
      "[3042 rows x 2 columns]\n",
      "     vidname  frame_50\n",
      "18    001458         0\n",
      "73    002482         0\n",
      "76    001451         0\n",
      "84    002644         0\n",
      "89    002449         0\n",
      "...      ...       ...\n",
      "1262    1263         1\n",
      "1272    1273         1\n",
      "1326    1327         1\n",
      "1399    1400         1\n",
      "1485    1486         1\n",
      "\n",
      "[537 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "'''train_df = data_new.sample(frac=0.75).astype(str)\n",
    "test_df = data_new.drop(train_df.index).astype(str)'''\n",
    "\n",
    "\n",
    "# Using DataFrame.sample() \n",
    "train_df=data_new.sample(frac=0.8,random_state=200)\n",
    "test_df=data_new.drop(train_df.index)\n",
    "\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ae7de1e4-4b8c-46b8-b8a9-1134d924d94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "path = \"/Users/nalthan/Desktop/vertiasopencvprojecy/\"\n",
    "normal_file_names = [path_normal + \"/\" + filename for filename in os.listdir(path_normal) if filename.endswith('.mp4')]\n",
    "\n",
    "'''for ind in train_df.index:\n",
    "    from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    if train_df.loc[ind, \"frame_50\"].where() == 1:\n",
    "        name = train_df.loc[ind, \"vidname\"]\n",
    "        length = 6 -len(name)\n",
    "        name = name.zfill(length)\n",
    "        print(name)\n",
    "        os.rename(path+\"Crash-1500/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "    if train_df.loc[ind, \"frame_50\"].item() == 0:\n",
    "        name = train_df.loc[ind, \"vidname\"]\n",
    "        print(name)\n",
    "        os.rename(path+\"Normal/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")'''\n",
    "'''for f in glob.glob(path+\"train\"):\n",
    "    print(f)\n",
    "    os.remove(f)\n",
    "for f in glob.glob(path+\"test\"):\n",
    "    print(f)\n",
    "    os.remove(f)'''\n",
    "\n",
    "for row in train_df.iterrows():\n",
    "    if str(row[1][0]) == \".DS_Store\":\n",
    "        continue\n",
    "    if row[1][1] == 1:\n",
    "        name = str(row[1][0])\n",
    "        name = name.zfill(6)\n",
    "        #os.rename(path+\"Crash-1500/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "        shutil.copy(path+\"Crash-1500/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "    else:\n",
    "        name = row[1][0]\n",
    "        #os.rename(path+\"Normal/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "        shutil.copy(path+\"Normal/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "        \n",
    "for row in test_df.iterrows():\n",
    "    if row[1][1] == 1:\n",
    "        name = str(row[1][0])\n",
    "        name = name.zfill(6)\n",
    "        #os.rename(path+\"Crash-1500/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "        shutil.copy(path+\"Crash-1500/\"+name+\".mp4\", path+\"test/\"+name+\".mp4\")\n",
    "    else:\n",
    "        name = row[1][0]\n",
    "        #os.rename(path+\"Normal/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "        shutil.copy(path+\"Normal/\"+name+\".mp4\", path+\"test/\"+name+\".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "07e83c2f-81b7-4abf-9fb0-fb16e008d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "41757df0-b2b3-4c2c-bcba-58bfd3de6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3af629-8778-41cc-9005-0ba7746b3da7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''label_processor = keras.layers.StringLookup(\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    num_oov_indices=0, vocabulary=np.unique(train_df.iloc[:, 1])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mprint(label_processor.get_vocabulary())'''\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_df)\n",
    "\n",
    "'''label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df.iloc[:, 1])\n",
    ")\n",
    "print(label_processor.get_vocabulary())'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e30d1974-8bca-4f3c-8546-ad4671072921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 106\n",
      "<class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@34800.447] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2386) handleMessage OpenCV | GStreamer warning: your GStreamer installation is missing a required plugin\n",
      "[ WARN:0@34800.447] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2402) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module uridecodebin3585 reported: Your GStreamer installation is missing a plug-in.\n",
      "[ WARN:0@34800.447] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (1356) open OpenCV | GStreamer warning: unable to start pipeline\n",
      "[ WARN:0@34800.447] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1 1139\n",
      "<class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@34802.086] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2386) handleMessage OpenCV | GStreamer warning: your GStreamer installation is missing a required plugin\n",
      "[ WARN:0@34802.086] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2402) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module uridecodebin3586 reported: Your GStreamer installation is missing a plug-in.\n",
      "[ WARN:0@34802.086] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (1356) open OpenCV | GStreamer warning: unable to start pipeline\n",
      "[ WARN:0@34802.086] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "2 001465\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@34803.450] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2386) handleMessage OpenCV | GStreamer warning: your GStreamer installation is missing a required plugin\n",
      "[ WARN:0@34803.450] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2402) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module uridecodebin3587 reported: Your GStreamer installation is missing a plug-in.\n",
      "[ WARN:0@34803.450] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (1356) open OpenCV | GStreamer warning: unable to start pipeline\n",
      "[ WARN:0@34803.450] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "3 000543\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@34804.699] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2386) handleMessage OpenCV | GStreamer warning: your GStreamer installation is missing a required plugin\n",
      "[ WARN:0@34804.699] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2402) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module uridecodebin3588 reported: Your GStreamer installation is missing a plug-in.\n",
      "[ WARN:0@34804.699] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (1356) open OpenCV | GStreamer warning: unable to start pipeline\n",
      "[ WARN:0@34804.699] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[321], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m         frame_masks[idx,] \u001b[38;5;241m=\u001b[39m temp_frame_mask\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (frame_features, frame_masks), labels\n\u001b[0;32m---> 44\u001b[0m train_data, train_labels \u001b[38;5;241m=\u001b[39m prepare_all_videos(train_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/nalthan/Desktop/vertiasopencvprojecy/train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m test_data, test_labels \u001b[38;5;241m=\u001b[39m prepare_all_videos(test_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/nalthan/Desktop/vertiasopencvprojecy/test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame features in train set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[321], line 33\u001b[0m, in \u001b[0;36mprepare_all_videos\u001b[0;34m(df, root_dir)\u001b[0m\n\u001b[1;32m     31\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(MAX_SEQ_LENGTH, video_length)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(length):\n\u001b[0;32m---> 33\u001b[0m         temp_frame_features[i, j, :] \u001b[38;5;241m=\u001b[39m feature_extractor\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     34\u001b[0m             batch[\u001b[38;5;28;01mNone\u001b[39;00m, j, :]\n\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m     temp_frame_mask[i, :length] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 1 = not masked, 0 = masked\u001b[39;00m\n\u001b[1;32m     38\u001b[0m frame_features[idx,] \u001b[38;5;241m=\u001b[39m temp_frame_features\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:2550\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2548\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[1;32m   2549\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2550\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1331\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1331\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset)\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:506\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterator_ops\u001b[38;5;241m.\u001b[39mOwnedIterator(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:710\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    706\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 710\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_iterator(dataset)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    747\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    748\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 749\u001b[0m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmake_iterator(ds_variant, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3420\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3419\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3420\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[1;32m   3421\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMakeIterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, dataset, iterator)\n\u001b[1;32m   3422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3423\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"vidname\"].values.tolist()\n",
    "    labels = df[\"frame_50\"].values\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(idx, path)\n",
    "        print(type(path))\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, str(path).zfill(6)+'.mp4'))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"/Users/nalthan/Desktop/vertiasopencvprojecy/train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"/Users/nalthan/Desktop/vertiasopencvprojecy/test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9541fc96-4fc5-4b86-922d-53eed0b20c92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history, seq_model\n\u001b[0;32m---> 48\u001b[0m _, sequence_model \u001b[38;5;241m=\u001b[39m run_experiment()\n",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_experiment\u001b[39m():\n\u001b[1;32m     27\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/video_classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     29\u001b[0m         filepath, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     seq_model \u001b[38;5;241m=\u001b[39m get_sequence_model()\n\u001b[1;32m     33\u001b[0m     history \u001b[38;5;241m=\u001b[39m seq_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     34\u001b[0m         [train_data[\u001b[38;5;241m0\u001b[39m], train_data[\u001b[38;5;241m1\u001b[39m]],\n\u001b[1;32m     35\u001b[0m         train_labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[checkpoint],\n\u001b[1;32m     39\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "87a5f131-e0f9-451a-b1d4-2a9e8e8a5e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: 001618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@36310.621] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2386) handleMessage OpenCV | GStreamer warning: your GStreamer installation is missing a required plugin\n",
      "[ WARN:0@36310.621] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (2402) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module uridecodebin3602 reported: Your GStreamer installation is missing a plug-in.\n",
      "[ WARN:0@36310.621] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (1356) open OpenCV | GStreamer warning: unable to start pipeline\n",
      "[ WARN:0@36310.621] global /Users/cbousseau/work/recipes/ci_py311/opencv-suite_1677933071594/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "  0: 53.34%\n",
      "  1: 46.66%\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "# This utility is for visualization.\n",
    "# Referenced from:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def to_gif(images):\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
    "    return embed.embed_file(\"animation.gif\")\n",
    "\n",
    "\n",
    "test_video = np.random.choice(test_df[\"vidname\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(\"/Users/nalthan/Desktop/vertiasopencvprojecy/test/001486.mp4\")\n",
    "#to_gif(test_frames[:MAX_SEQ_LENGTH])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "03b61bb6-0475-4898-a8bd-40a7f3613bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.55690026 0.00372693 1.164155   ... 0.5683828  0.2273449  0.27284822]\n",
      "  [0.5518104  0.         0.56378764 ... 0.32555878 0.20508468 0.37070766]\n",
      "  [0.7172964  0.         0.57053643 ... 0.4331803  0.16984011 0.34802505]\n",
      "  ...\n",
      "  [0.5535542  0.12226056 0.30052516 ... 0.8323538  0.24497993 0.02150873]\n",
      "  [0.7964966  0.258994   0.20246062 ... 0.9666561  0.43305612 0.15012342]\n",
      "  [0.42056656 0.23802823 0.12534954 ... 1.0736641  0.2836176  0.06860691]]\n",
      "\n",
      " [[0.9625615  0.13510786 0.6706079  ... 0.21455297 0.21543796 0.9329506 ]\n",
      "  [0.86544126 0.09389149 0.6420261  ... 0.46122724 0.10112777 1.0568451 ]\n",
      "  [1.147747   0.1785585  0.69596744 ... 0.47264764 0.05745816 0.9192556 ]\n",
      "  ...\n",
      "  [0.8775185  0.2427949  0.5206336  ... 0.18757854 0.17429346 0.60987484]\n",
      "  [0.7811601  0.14033282 0.5562525  ... 0.27848884 0.5472677  0.54624724]\n",
      "  [0.77163225 0.13750356 0.57990557 ... 0.30191937 0.66745603 0.7738948 ]]\n",
      "\n",
      " [[0.5100756  0.17153814 0.71176195 ... 0.39772078 0.4923519  0.6491169 ]\n",
      "  [0.23585825 0.09509332 0.51756644 ... 0.40605053 0.985397   0.8725479 ]\n",
      "  [0.28257352 0.19092852 0.3984895  ... 0.6106352  0.72211045 0.59505445]\n",
      "  ...\n",
      "  [0.4413034  0.20865937 0.35924533 ... 1.2253032  0.8601099  1.1702026 ]\n",
      "  [0.68306357 0.08631769 0.21241052 ... 1.2230211  1.0996867  0.35938388]\n",
      "  [0.56859624 0.0109335  0.41551077 ... 1.1254776  0.8720062  0.7525111 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.54557806 0.12301792 0.43084237 ... 2.522813   0.12393428 1.6060882 ]\n",
      "  [0.47847515 0.27526832 0.46983227 ... 2.4688778  0.3197579  1.8062027 ]\n",
      "  [0.42717236 0.16085325 0.40739545 ... 2.2473278  0.19589949 1.5249858 ]\n",
      "  ...\n",
      "  [0.7464955  0.60633546 0.6311802  ... 1.7963108  0.32650664 1.3723862 ]\n",
      "  [0.5627084  0.49264497 0.4490985  ... 1.5756916  0.30985168 1.3196124 ]\n",
      "  [0.50548995 0.1918515  0.59620804 ... 1.8822992  0.29640874 1.0695567 ]]\n",
      "\n",
      " [[0.5195574  0.10489038 0.28790388 ... 0.6018755  0.00850757 0.04968393]\n",
      "  [0.330382   0.32546845 0.167167   ... 0.6744254  0.         0.37015232]\n",
      "  [0.43425313 0.20563962 0.09514809 ... 0.60963666 0.01300343 0.6096439 ]\n",
      "  ...\n",
      "  [0.607621   0.1348092  0.8075637  ... 0.44483116 0.08375213 0.31057   ]\n",
      "  [0.42947438 0.14726624 0.6056023  ... 0.68714213 0.25547305 0.21342655]\n",
      "  [0.5295173  0.34483674 0.56093156 ... 0.33509964 0.14540546 0.75316846]]\n",
      "\n",
      " [[0.78401977 0.19826351 1.0030191  ... 0.10724136 0.16449772 0.04468638]\n",
      "  [0.79811347 0.11385525 1.5118717  ... 0.37987638 0.17752129 0.06979964]\n",
      "  [0.8031958  0.19980118 1.2173485  ... 0.27078134 0.06060264 0.10663779]\n",
      "  ...\n",
      "  [0.40361032 0.23593213 1.0797125  ... 0.93334305 0.6470283  0.22049886]\n",
      "  [0.29011568 0.24234377 0.89992744 ... 0.561501   0.5811195  0.35227704]\n",
      "  [0.58126897 0.39718598 0.93333673 ... 0.4661794  0.74801993 0.25378054]]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630539e-2f3c-4e9d-ae9c-9c1ff5918067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
