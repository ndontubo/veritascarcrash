{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94449d0-1cf8-4691-ac47-b110e2eb2764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2393a0b-08c1-429e-8eac-4f395d284f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f17be8-d9dd-4664-9755-171f83d2da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa125753-f66c-4006-9cef-c124ce9396d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552fab26-1a3d-4197-be95-e1df22eb4925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f3b413-7b75-4504-b91e-cfd6c233b23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "train_df = train_df.iloc[: , 1:]\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "test_df = test_df.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae7de1e4-4b8c-46b8-b8a9-1134d924d94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "path = \"/Users/nalthan/Desktop/vertiasopencvprojecy/\"\n",
    "normal_file_names = [path_normal + \"/\" + filename for filename in os.listdir(path) if filename.endswith('.mp4')]\n",
    "\n",
    "'''for ind in train_df.index:\n",
    "    from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    if train_df.loc[ind, \"frame_50\"].where() == 1:\n",
    "        name = train_df.loc[ind, \"vidname\"]\n",
    "        length = 6 -len(name)\n",
    "        name = name.zfill(length)\n",
    "        print(name)\n",
    "        os.rename(path+\"Crash-1500/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "    if train_df.loc[ind, \"frame_50\"].item() == 0:\n",
    "        name = train_df.loc[ind, \"vidname\"]\n",
    "        print(name)\n",
    "        os.rename(path+\"Normal/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")'''\n",
    "'''for f in glob.glob(path+\"train\"):\n",
    "    print(f)\n",
    "    os.remove(f)\n",
    "for f in glob.glob(path+\"test\"):\n",
    "    print(f)\n",
    "    os.remove(f)'''\n",
    "\n",
    "for row in train_df.iterrows():\n",
    "    if str(row[1][0]) == \".DS_Store\":\n",
    "        continue\n",
    "    name = str(row[1][0])\n",
    "    name = name.zfill(6)\n",
    "    if row[1][1] == 1:\n",
    "        shutil.copy(path+\"Crash-1500/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "    else:\n",
    "        shutil.copy(path+\"Normal/\"+name+\".mp4\", path+\"train/\"+name+\".mp4\")\n",
    "        \n",
    "for row in test_df.iterrows():\n",
    "    name = str(row[1][0])\n",
    "    name = name.zfill(6)\n",
    "    if row[1][1] == 1:\n",
    "        shutil.copy(path+\"Crash-1500/\"+name+\".mp4\", path+\"test/\"+name+\".mp4\")\n",
    "    else:\n",
    "        shutil.copy(path+\"Normal/\"+name+\".mp4\", path+\"test/\"+name+\".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb3af629-8778-41cc-9005-0ba7746b3da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nalthan/anaconda3/lib/python3.11/site-packages/numpy/core/numeric.py:2468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=tf.convert_to_tensor(np.unique(train_df[\"frame_50\"].astype(str).tolist())))\n",
    "\n",
    "print(label_processor.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d1974-8bca-4f3c-8546-ad4671072921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"vidname\"].values.tolist()\n",
    "    labels = df[\"frame_50\"].values\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(idx, path)\n",
    "        print(type(path))\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, str(path).zfill(6)+'.mp4'))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"/Users/nalthan/Desktop/vertiasopencvprojecy/train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"/Users/nalthan/Desktop/vertiasopencvprojecy/test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95006133-ba5d-43cc-8937-4a1bb5949194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/67 [===========================>..] - ETA: 0s - loss: 0.5381 - accuracy: 0.7822\n",
      "Epoch 1: val_loss improved from inf to 0.47573, saving model to /tmp/video_classifier\n",
      "67/67 [==============================] - 22s 128ms/step - loss: 0.5376 - accuracy: 0.7820 - val_loss: 0.4757 - val_accuracy: 0.7864\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.5205 - accuracy: 0.7843\n",
      "Epoch 2: val_loss improved from 0.47573 to 0.44095, saving model to /tmp/video_classifier\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.5205 - accuracy: 0.7843 - val_loss: 0.4409 - val_accuracy: 0.7864\n",
      "Epoch 3/10\n",
      "64/67 [===========================>..] - ETA: 0s - loss: 0.5054 - accuracy: 0.7891\n",
      "Epoch 3: val_loss improved from 0.44095 to 0.43681, saving model to /tmp/video_classifier\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.5073 - accuracy: 0.7876 - val_loss: 0.4368 - val_accuracy: 0.7864\n",
      "Epoch 4/10\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.7841\n",
      "Epoch 4: val_loss improved from 0.43681 to 0.42590, saving model to /tmp/video_classifier\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.5043 - accuracy: 0.7843 - val_loss: 0.4259 - val_accuracy: 0.7864\n",
      "Epoch 5/10\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.7879\n",
      "Epoch 5: val_loss improved from 0.42590 to 0.39159, saving model to /tmp/video_classifier\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.4911 - accuracy: 0.7867 - val_loss: 0.3916 - val_accuracy: 0.7864\n",
      "Epoch 6/10\n",
      "64/67 [===========================>..] - ETA: 0s - loss: 0.4861 - accuracy: 0.7881\n",
      "Epoch 6: val_loss did not improve from 0.39159\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 0.4887 - accuracy: 0.7867 - val_loss: 0.4669 - val_accuracy: 0.7864\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.7867\n",
      "Epoch 7: val_loss did not improve from 0.39159\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.4878 - accuracy: 0.7867 - val_loss: 0.4501 - val_accuracy: 0.7864\n",
      "Epoch 8/10\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.4935 - accuracy: 0.7875\n",
      "Epoch 8: val_loss did not improve from 0.39159\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.4944 - accuracy: 0.7867 - val_loss: 0.4311 - val_accuracy: 0.7864\n",
      "Epoch 9/10\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.4879 - accuracy: 0.7885\n",
      "Epoch 9: val_loss did not improve from 0.39159\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.4894 - accuracy: 0.7871 - val_loss: 0.4313 - val_accuracy: 0.7864\n",
      "Epoch 10/10\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.4806 - accuracy: 0.7879\n",
      "Epoch 10: val_loss did not improve from 0.39159\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.4810 - accuracy: 0.7871 - val_loss: 0.4238 - val_accuracy: 0.7864\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2657 - accuracy: 0.9250\n",
      "Test accuracy: 92.5%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "87a5f131-e0f9-451a-b1d4-2a9e8e8a5e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: 001465\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    return [round(probabilities[0]*100, 2), round(probabilities[1]*100, 2)] \n",
    "\n",
    "\n",
    "\n",
    "test_video = str(np.random.choice(test_df[\"vidname\"].values.tolist())).zfill(6)\n",
    "print(f\"Test video path: {test_video}\")\n",
    "#test_frames = sequence_prediction(str(test_video)+\".mp4\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6604232-db56-4846-acc9-983faf85f5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vidname = []\n",
    "real = []\n",
    "predict0 = []\n",
    "predict1 = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    vidname.append(str(row[\"vidname\"]).zfill(6))\n",
    "    real.append(row[\"frame_50\"])\n",
    "    percentages = []\n",
    "    percentages = sequence_prediction(str(row[\"vidname\"]).zfill(6)+\".mp4\")\n",
    "    predict0.append(percentages[0])\n",
    "    predict1.append(percentages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92279417-797d-4c33-8cbe-9e51d401e380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict = {'vidname': vidname, 'real': real, 'predict0': predict0, 'predict1':predict1} \n",
    "    \n",
    "results = pd.DataFrame(dict)\n",
    "results.to_csv('inception_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33805e8-011b-4c5d-beec-bbe816aa4e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
